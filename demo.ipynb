{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install gradio\n",
        "%%capture\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "fB8mrxXNi6cz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv8P9nUeihCc",
        "outputId": "6971cbce-9d54-480e-ebb3-79d585b0478e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import everything that is needed\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.nn.functional import softmax"
      ],
      "metadata": {
        "id": "uACXYpXJj0A4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models and tokenizer\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "anti_expert_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "expert_model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Colab_Notebooks/Transformers/fine_tuned_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move models to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "anti_expert_model.to(device)\n",
        "expert_model.to(device)\n",
        "\n",
        "# Proxy-Tuning Function\n",
        "def generate_answers(question):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Compute logits for all models\n",
        "    with torch.no_grad():\n",
        "        base_logits = base_model(**inputs).logits\n",
        "        anti_expert_logits = anti_expert_model(**inputs).logits\n",
        "        expert_logits = expert_model(**inputs).logits\n",
        "\n",
        "    # Proxy-tuned logits\n",
        "    alpha = 2.0  # Adjust to see what works best\n",
        "    proxy_tuned_logits = base_logits + alpha * (expert_logits - anti_expert_logits)\n",
        "\n",
        "    # Generate GPT-2 answer\n",
        "    base_answer_ids = torch.argmax(softmax(base_logits, dim=-1), dim=-1)\n",
        "    gpt2_answer = tokenizer.decode(base_answer_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate Proxy-Tuned answer\n",
        "    proxy_tuned_answer_ids = torch.argmax(softmax(proxy_tuned_logits, dim=-1), dim=-1)\n",
        "    proxy_tuned_answer = tokenizer.decode(proxy_tuned_answer_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return gpt2_answer, proxy_tuned_answer\n",
        "\n",
        "# Gradio Interface\n",
        "def process_input(question):\n",
        "    gpt2_answer, proxy_tuned_answer = generate_answers(question)\n",
        "    return f\"GPT-2 Answer: {gpt2_answer}\\nProxy-Tuned Answer: {proxy_tuned_answer}\"\n",
        "\n",
        "# Set up Gradio app\n",
        "interface = gr.Interface(\n",
        "    fn=process_input,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Proxy-Tuning Demonstration\",\n",
        "    description=\"Enter a question and see answers generated by GPT-2 and Proxy-Tuned GPT-2.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio interface\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "JVV5RIH-j0HJ",
        "outputId": "c72a7a13-ce58-4784-ee2d-c53dad701b5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://075c90ca4e06505f1e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://075c90ca4e06505f1e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNdKECQHkaCK"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}